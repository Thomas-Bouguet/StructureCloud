{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.1.14:4040\n",
       "SparkContext available as 'sc' (version = 3.4.1, master = local[*], app id = local-1700767270793)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.{SparkSession, functions=>F}\r\n",
       "old_data_path: String = C:/Thomas/Etudes/ESILV/A9/Structure_de_donnees_cloud/Projet/Report_4/startingTables/\r\n",
       "new_data_path: String = C:/Thomas/Etudes/ESILV/A9/Structure_de_donnees_cloud/Projet/Report_4/denTables/\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.{SparkSession, functions => F}\n",
    "\n",
    "var old_data_path = \"C:/Thomas/Etudes/ESILV/A9/Structure_de_donnees_cloud/Projet/Report_4/startingTables/\"\n",
    "var new_data_path = \"C:/Thomas/Etudes/ESILV/A9/Structure_de_donnees_cloud/Projet/Report_4/denTables/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@6abe9311\r\n",
       "moviesDF: org.apache.spark.sql.DataFrame = [id: string, name: string ... 2 more fields]\r\n",
       "oldMoviesGenresDF: org.apache.spark.sql.DataFrame = [movie_id: string, genre: string]\r\n",
       "oldMoviesDirectorsDF: org.apache.spark.sql.DataFrame = [director_id: string, movie_id: string]\r\n",
       "oldRolesDF: org.apache.spark.sql.DataFrame = [actor_id: string, movie_id: string ... 1 more field]\r\n",
       "directorsDF: org.apache.spark.sql.DataFrame = [id: string, first_name: string ... 1 more field]\r\n",
       "oldDirectorsGenresDF: org.apache.spark.sql.DataFrame = [director_id: string, genre: string ... 1 more field]\r\n",
       "actorsDF: org.apache.spark.sql.DataFrame = [id: string, first_name: string ... 2 more fields]\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create a Spark session\n",
    "val spark = SparkSession.builder.appName(\"Denormalization\")\n",
    "  .config(\"spark.driver.memory\", \"8g\")\n",
    "  .config(\"spark.executor.memory\", \"8g\")\n",
    "  .getOrCreate()\n",
    "\n",
    "// Load the CSV data into DataFrames\n",
    "val moviesDF = spark.read.csv(old_data_path + \"movies.csv\").toDF(\"id\",\"name\",\"year\",\"rank\")\n",
    "val oldMoviesGenresDF = spark.read.csv(old_data_path + \"movies_genres.csv\").toDF(\"movie_id\",\"genre\")\n",
    "val oldMoviesDirectorsDF = spark.read.csv(old_data_path + \"movies_directors.csv\").toDF(\"director_id\",\"movie_id\")\n",
    "val oldRolesDF = spark.read.csv(old_data_path + \"roles.csv\").toDF(\"actor_id\",\"movie_id\",\"role\")\n",
    "\n",
    "val directorsDF = spark.read.csv(old_data_path + \"directors.csv\").toDF(\"id\", \"first_name\", \"last_name\")\n",
    "val oldDirectorsGenresDF = spark.read.csv(old_data_path + \"directors_genres.csv\").toDF(\"director_id\",\"genre\",\"prob\")\n",
    "\n",
    "val actorsDF = spark.read.csv(old_data_path + \"actors.csv\").toDF(\"id\",\"first_name\",\"last_name\",\"gender\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "denormalizedMoviesDF: org.apache.spark.sql.DataFrame = [id: string, name: string ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Denormalize movies.list_genres\n",
    "val denormalizedMoviesDF = moviesDF\n",
    "  .join(oldMoviesGenresDF, moviesDF(\"id\") === oldMoviesGenresDF(\"movie_id\"), \"left_outer\")\n",
    "  .groupBy(\"id\", \"name\", \"year\", \"rank\")\n",
    "  .agg(F.collect_list(\"genre\").alias(\"list_genres\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "denormalizedMoviesWithDirectorsDF: org.apache.spark.sql.DataFrame = [id: string, name: string ... 4 more fields]\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Denormalize movies.list_directors_id\n",
    "val denormalizedMoviesWithDirectorsDF = denormalizedMoviesDF\n",
    "  .join(oldMoviesDirectorsDF, denormalizedMoviesDF(\"id\") === oldMoviesDirectorsDF(\"movie_id\"), \"left_outer\")\n",
    "  .groupBy(\"id\", \"name\", \"year\", \"rank\", \"list_genres\")\n",
    "  .agg(F.collect_list(\"director_id\").alias(\"list_directors_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "denormalizedMoviesWithActorsDF: org.apache.spark.sql.DataFrame = [id: string, name: string ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Denormalize movies.den_nb_actors\n",
    "val denormalizedMoviesWithActorsDF = denormalizedMoviesWithDirectorsDF\n",
    "  .join(oldRolesDF, denormalizedMoviesWithDirectorsDF(\"id\") === oldRolesDF(\"movie_id\"), \"left_outer\")\n",
    "  .groupBy(\"id\", \"name\", \"year\", \"rank\", \"list_genres\", \"list_directors_id\")\n",
    "  .agg(F.size(F.collect_list(\"actor_id\")).alias(\"den_nb_actors\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "denormalizedDirectorsWithMoviesDF: org.apache.spark.sql.DataFrame = [id: string, first_name: string ... 2 more fields]\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Denormalize directors.list_movies_id\n",
    "val denormalizedDirectorsWithMoviesDF = directorsDF\n",
    "  .join(oldMoviesDirectorsDF, directorsDF(\"id\") === oldMoviesDirectorsDF(\"director_id\"),\"left_outer\")\n",
    "  .groupBy(\"id\",\"first_name\",\"last_name\")\n",
    "  .agg(F.collect_list(\"movie_id\").alias(\"list_movies_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "denormalizedDirectorsDF: org.apache.spark.sql.DataFrame = [id: string, first_name: string ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Denormalize directors.dict_genres_den_prob\n",
    "val denormalizedDirectorsDF = denormalizedDirectorsWithMoviesDF\n",
    "  .join(oldDirectorsGenresDF, denormalizedDirectorsWithMoviesDF(\"id\") === oldDirectorsGenresDF(\"director_id\"), \"left_outer\")\n",
    "  .groupBy(\"id\", \"first_name\", \"last_name\", \"list_movies_id\")\n",
    "  .agg(\n",
    "    F.map_from_entries(\n",
    "      F.collect_list(\n",
    "        F.when(\n",
    "          F.col(\"genre\").isNotNull && F.col(\"prob\").isNotNull,\n",
    "          F.struct(\"genre\", \"prob\")\n",
    "        )\n",
    "      )\n",
    "    ).alias(\"dict_genres_den_prob\")\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "denormalizedActorsDF: org.apache.spark.sql.DataFrame = [id: string, first_name: string ... 2 more fields]\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Denormalize actors.list_movies_id\n",
    "val denormalizedActorsDF = actorsDF\n",
    "  .join(oldRolesDF, actorsDF(\"id\") === oldRolesDF(\"actor_id\"), \"left_outer\")\n",
    "  .groupBy(\"id\", \"first_name\", \"last_name\")\n",
    "  .agg(F.collect_list(\"movie_id\").alias(\"list_movies_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Save the denormalized data to a JSON file\n",
    "denormalizedActorsDF.write.mode(\"overwrite\").json(new_data_path + \"denormalized_actors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Save the denormalized data to a JSON file\n",
    "denormalizedDirectorsDF.write\n",
    "  .mode(\"overwrite\") // Use \"overwrite\" or \"append\" based on your requirement\n",
    "  .json(new_data_path + \"denormalized_directors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57.615s][warning][gc,alloc] Executor task launch worker for task 0.0 in stage 26.0 (TID 114): Retried waiting for GCLocker too often allocating 131074 words\n",
      "[57.616s][warning][gc,alloc] Executor task launch worker for task 8.0 in stage 26.0 (TID 122): Retried waiting for GCLocker too often allocating 131074 words\n",
      "[57.620s][warning][gc,alloc] Executor task launch worker for task 13.0 in stage 26.0 (TID 127): Retried waiting for GCLocker too often allocating 131074 words\n"
     ]
    },
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted due to stage failure: Task 0 in stage 26.0 failed 1 times, most recent failure: Lost task 0.0 in stage 26.0 (TID 114) (192.168.1.14 executor driver): java.lang.OutOfMemoryError: Java heap space\r",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 26.0 failed 1 times, most recent failure: Lost task 0.0 in stage 26.0 (TID 114) (192.168.1.14 executor driver): java.lang.OutOfMemoryError: Java heap space\r",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:50)\r",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\r",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:555)\r",
      "\tat org.apache.spark.sql.execution.UnsafeKVExternalSorter.sortedIterator(UnsafeKVExternalSorter.java:206)\r",
      "\tat org.apache.spark.sql.execution.aggregate.SortBasedAggregator$$anon$1.<init>(ObjectAggregationIterator.scala:250)\r",
      "\tat org.apache.spark.sql.execution.aggregate.SortBasedAggregator.destructiveIterator(ObjectAggregationIterator.scala:249)\r",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.processInputs(ObjectAggregationIterator.scala:208)\r",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.<init>(ObjectAggregationIterator.scala:83)\r",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:114)\r",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\r",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec$$Lambda$5136/0x0000000801dfe5a8.apply(Unknown Source)\r",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:875)\r",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:875)\r",
      "\tat org.apache.spark.rdd.RDD$$Lambda$4230/0x0000000801ce5d88.apply(Unknown Source)\r",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$3499/0x0000000801b76000.apply(Unknown Source)\r",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r",
      "\r",
      "Driver stacktrace:\r",
      "  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r",
      "  at scala.Option.foreach(Option.scala:407)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r",
      "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\r",
      "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\r",
      "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r",
      "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r",
      "  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:354)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:382)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:354)\r",
      "  at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r",
      "  at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r",
      "  at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r",
      "  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r",
      "  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r",
      "  at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r",
      "  at org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:771)\r",
      "  ... 38 elided\r",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\r",
      "  at org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:50)\r",
      "  at org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\r",
      "  at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:555)\r",
      "  at org.apache.spark.sql.execution.UnsafeKVExternalSorter.sortedIterator(UnsafeKVExternalSorter.java:206)\r",
      "  at org.apache.spark.sql.execution.aggregate.SortBasedAggregator$$anon$1.<init>(ObjectAggregationIterator.scala:250)\r",
      "  at org.apache.spark.sql.execution.aggregate.SortBasedAggregator.destructiveIterator(ObjectAggregationIterator.scala:249)\r",
      "  at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.processInputs(ObjectAggregationIterator.scala:208)\r",
      "  at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.<init>(ObjectAggregationIterator.scala:83)\r",
      "  at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:114)\r",
      "  at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\r",
      "  at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec$$Lambda$5136/0x0000000801dfe5a8.apply(Unknown Source)\r",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:875)\r",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:875)\r",
      "  at org.apache.spark.rdd.RDD$$Lambda$4230/0x0000000801ce5d88.apply(Unknown Source)\r",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r",
      "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r",
      "  at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:139)\r",
      "  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r",
      "  at org.apache.spark.executor.Executor$TaskRunner$$Lambda$3499/0x0000000801b76000.apply(Unknown Source)\r",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r",
      "  at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r",
      "  at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r",
      ""
     ]
    }
   ],
   "source": [
    "// Save the denormalized data to a JSON file\n",
    "denormalizedMoviesWithActorsDF.write\n",
    "  .mode(\"overwrite\") // Use \"overwrite\" or \"append\" based on your requirement\n",
    "  .json(new_data_path + \"denormalized_movies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Stop the Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
